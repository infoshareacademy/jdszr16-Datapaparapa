{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def hopkins_statistic(data, sample_size=50, random_state=None):\n",
    "    \"\"\"\n",
    "    Oblicza statystykę Hopkinsa dla danego zbioru danych.\n",
    "    \n",
    "    Parameters:\n",
    "        data (numpy.ndarray or pandas.DataFrame): Dane do analizy.\n",
    "        sample_size (int): Liczba próbek do wylosowania.\n",
    "        random_state (int or None): Losowe ziarno (opcjonalne).\n",
    "        \n",
    "    Returns:\n",
    "        float: Wartość statystyki Hopkinsa (0 - dane losowe, 1 - dane klastrowe).\n",
    "    \"\"\"\n",
    "    if isinstance(data, np.ndarray):\n",
    "        X = data\n",
    "    else:\n",
    "        X = data.to_numpy()\n",
    "    \n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    n, d = X.shape\n",
    "    if sample_size > n:\n",
    "        raise ValueError(\"Sample size cannot exceed number of data points.\")\n",
    "    \n",
    "    # Losowanie próbek z danych\n",
    "    indices = np.random.choice(n, sample_size, replace=False)\n",
    "    samples = X[indices]\n",
    "    \n",
    "    # Generowanie losowych punktów w tym samym zakresie\n",
    "    random_points = np.random.uniform(\n",
    "        np.min(X, axis=0), np.max(X, axis=0), (sample_size, d)\n",
    "    )\n",
    "    \n",
    "    # Dopasowanie modelu Nearest Neighbors\n",
    "    nbrs = NearestNeighbors(n_neighbors=2).fit(X)\n",
    "    \n",
    "    # Odległości dla próbek z danych\n",
    "    distances_real, _ = nbrs.kneighbors(samples)\n",
    "    W = np.sum(distances_real[:, 1])  # 2 najbliższy punkt\n",
    "    \n",
    "    # Odległości dla losowych punktów\n",
    "    distances_random, _ = nbrs.kneighbors(random_points)\n",
    "    U = np.sum(distances_random[:, 0])  # 1 najbliższy punkt\n",
    "    \n",
    "    # Statystyka Hopkinsa\n",
    "    H = W / (W + U)\n",
    "    return H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statystyka Hopkinsa: 0.011788158608951263\n",
      "Dane są rozproszone, klastrowanie może być mniej efektywne.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "\n",
    "rfm_data = pd.read_csv('analiza_rfm.csv')\n",
    "\n",
    "# Załóżmy, że analiza_rfm zawiera kolumny 'Recency', 'Frequency', 'Monetary'\n",
    "\n",
    "# 1. Normalizacja danych\n",
    "scaler = MinMaxScaler()\n",
    "rfm_normalized = scaler.fit_transform(rfm_data)\n",
    "\n",
    "# 2. Obliczenie statystyki Hopkinsa\n",
    "H = hopkins_statistic(rfm_normalized, sample_size=50, random_state=42)  # Użyj wcześniej zaimplementowanej funkcji\n",
    "print(f\"Statystyka Hopkinsa: {H}\")\n",
    "\n",
    "# Interpretacja wyniku\n",
    "if H > 0.75:\n",
    "    print(\"Dane są wysoce skupiskowe, nadają się do klastrowania.\")\n",
    "elif 0.5 < H <= 0.75:\n",
    "    print(\"Dane mogą mieć pewną strukturę skupiskową.\")\n",
    "else:\n",
    "    print(\"Dane są rozproszone, klastrowanie może być mniej efektywne.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
